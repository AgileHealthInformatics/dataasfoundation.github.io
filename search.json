[
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Articles",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "initiatives.html",
    "href": "initiatives.html",
    "title": "Initiatives",
    "section": "",
    "text": "[Metadata Registry Blueprint] – a reference model for NHS-scale registries.\nPractical Semantics Working Group – community profiles & validation tools."
  },
  {
    "objectID": "initiatives.html#active",
    "href": "initiatives.html#active",
    "title": "Initiatives",
    "section": "",
    "text": "[Metadata Registry Blueprint] – a reference model for NHS-scale registries.\nPractical Semantics Working Group – community profiles & validation tools."
  },
  {
    "objectID": "initiatives.html#in-development",
    "href": "initiatives.html#in-development",
    "title": "Initiatives",
    "section": "In development",
    "text": "In development\n\nAI Assurance Patterns for Health – governance recipes grounded in data foundations.\n\nInterested in collaborating? Get in touch."
  },
  {
    "objectID": "events.html",
    "href": "events.html",
    "title": "Events",
    "section": "",
    "text": "2025\n\nData as Foundation: Launch webinar — Date TBA\n\nBCS Health & Care talk — Title TBA\n\n\nWant me to speak at your event? Reach out via the footer contact link."
  },
  {
    "objectID": "book/part5.html",
    "href": "book/part5.html",
    "title": "Part V — Enabling the Future of Healthcare",
    "section": "",
    "text": "Buy on Amazon\n\n\nThis part connects foundations to the frontier. It starts with Foundations for AI: trustworthy systems require governed data, explicit semantics, lineage, and human-in-the-loop oversight. Generative tools add capability but increase the need for documented context, prompt/response traceability, and model risk management aligned to emerging standards.\nNext is Data Management Maturity. Digital maturity (features, adoption) is not the same as capability maturity (definitions, stewardship, quality, metadata, lifecycle). A practical maturity model lets organisations baseline where they are, prioritise gaps, and invest sequentially—often starting with stewardship, registries, and quality services that unlock downstream value faster than another app rollout.\nFinally, The Cultural Shift. Durable change is behavioural: moving from ownership to stewardship, from gatekeeping to clarity, and from individual heroics to systematic improvement. Leaders signal priorities by funding foundations, publishing definitions, and celebrating quality wins. Literacy matters too: clinicians and managers don’t need to be ontologists, but they do need shared language about meaning, quality and risk.\nThe message is hopeful: with foundations in place, AI augments clinical judgment, analytics informs planning, and information flows safely through a governed ecosystem. Without them, every innovation amplifies inconsistency."
  },
  {
    "objectID": "book/part3.html",
    "href": "book/part3.html",
    "title": "Part III — Rethinking Healthcare Data",
    "section": "",
    "text": "Buy on Amazon\n\n\nHere the vocabulary changes. Most so-called “reuse” of clinical data is actually repurposing—an active transformation from one context to another. That shift matters. Repurposing requires explicit assessment of original context, designed transformations, validation against the new purpose, and end-to-end provenance.\nThe part maps common scenarios: clinical-to-research (narrative to variables, cohort definition, de-identification); admin-to-quality (reinterpreting billing codes, aligning timestamps to clinical workflow); individual-to-population (aggregation, geocoding, enrichment with social data); and clinical-to-AI (labeling, bias checks, representativeness). Each demands different quality thresholds and exposes hidden biases if context is missing.\nIt then argues that data is a dynamic asset. Its value appreciates through use: relationships are formed, interpretations layered, quality refined, and metadata enriched. Treating data as “at rest” underestimates both risk and opportunity. Governance must therefore be evolutionary—tracking lineage, propagating corrections upstream, and reassessing value and quality as purposes change.\nThe takeaway is pragmatic: replace “copy once, use everywhere” with “transform deliberately, prove fitness.” Build shared transformation components, document them like code, measure semantic completeness (not just format compliance), and publish provenance with outputs. That’s how organisations move safely from siloed records to a learning system."
  },
  {
    "objectID": "book/part1.html",
    "href": "book/part1.html",
    "title": "Part I — The Healthcare Data Paradox",
    "section": "",
    "text": "Buy on Amazon\n\nPart I investigates why health systems repeatedly invest in digital programmes that promise transformation yet deliver only partial gains. The central claim is stark: we optimised visible technology while neglecting the invisible foundation—data governance, metadata, and shared semantics. The paradox is that we have standards, frameworks and budgets, yet meaning still fractures as information crosses organisational and system boundaries.\nThe narrative starts with NHS history: departmental systems in the 1990s, the ambition of NPfIT, and later waves favouring local innovation. Each era improved tooling, but too often assumed that consistent, high-quality data would naturally follow. In practice, variation in coding, local extensions, and undocumented context created a brittleness that no amount of messaging middleware could fix. Technical interoperability advanced faster than semantic interoperability.\nThe part names a persistent confusion: information governance (lawful, secure handling) is not data governance (stewardship of meaning, quality and lifecycle). Organisations passed audits yet struggled to use shared data safely because definitions, provenance and quality were unclear. COVID-era acceleration amplified this: where foundations existed, analytics and coordination flourished; where they didn’t, dashboards and decision support were hampered by inconsistency and missing context.\nA second thread is the metadata gap. Clinical terminologies (e.g., SNOMED CT), classifications and modern exchange standards are necessary but insufficient without operational metadata that records how data was captured, by whom, using what method, with which units and reference ranges. Without it, repurposing for research, quality, or AI becomes risky guesswork.\nPart I concludes with a call to action: treat data capability as distinct from digitisation. Establish senior data leadership; fund metadata infrastructure (registries, lineage, quality services); and measure data management maturity alongside system adoption. The lesson from decades of effort is not to spend less on digital—but to sequence and govern differently so technology sits on bedrock, not sand."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Concepts",
    "section": "",
    "text": "This is the living, web-native companion to the book Data as Foundation.\nUse it to explore:\n\nData vs Information Governance — why compliance isn’t capability.\nMetadata as the missing linchpin — context that preserves meaning.\nSemantic interoperability in practice — implementable semantics for real teams.\nRepurposing, not reuse — moving data safely across contexts.\n\n\nHave a question or want to propose a correction? Open an issue on GitHub or use the contact link in the footer."
  },
  {
    "objectID": "book/overview.html",
    "href": "book/overview.html",
    "title": "About the Book",
    "section": "",
    "text": "Buy on Amazon\n\nData as Foundation: Building Healthcare’s Invisible Infrastructure argues that the real engine of digital health isn’t shiny apps or platforms—it’s the meaning baked into data and the governance that preserves it as information moves between contexts. Drawing on NHS history and international practice, the book shows why decades of digitisation have under-delivered: we automated processes but neglected the substrate that makes information trustworthy, reusable and safe.\nThe book is organised into six parts. Part I (The Healthcare Data Paradox) explains why large programmes faltered: they treated systems as ends, not means, and assumed good data would “emerge” from implementation. It introduces the blind spots around semantic consistency, metadata and true data governance. Part II (Fundamentals of Data Management) lays the core disciplines—data vs information governance, multidimensional data quality, and the pivotal role of metadata. It reframes quality as “fitness for purpose,” and metadata as operational infrastructure, not afterthought.\nPart III (Rethinking Healthcare Data) changes the vocabulary of improvement: most “data reuse” in health is actually repurposing that requires explicit transformation, provenance, and purpose-specific quality checks. It also treats data as a dynamic asset whose value and meaning evolve through use, relationships and enrichment. Part IV (Building the Infrastructure) moves from principles to architecture: standards ecosystems, metadata registries, and process documentation as the connective tissue for federated systems.\nLooking forward, Part V (Enabling the Future of Healthcare) shows that trustworthy AI depends on these same foundations—explainable models, auditable data, and an organisation that measures data management maturity alongside digital maturity. It also addresses the cultural work: leadership, stewardship and literacy. Finally, Part VI (Case Studies & Implementation) distils lessons from other sectors and offers practical, maturity-based steps for programmes such as Shared Care Records, while mapping future horizons like ecosystem governance and algorithmic assurance.\nAcross all six parts, the theme is constant: treat data as a managed, governed, semantic resource. Build registries and processes that preserve context. Separate compliance (information governance) from capability (data governance). Invest in metadata the way we invest in EPRs. Do this and digital health becomes steadily safer, more intelligent, and easier to change—because the foundation is sound."
  },
  {
    "objectID": "book/part2.html",
    "href": "book/part2.html",
    "title": "Part II — Fundamentals of Data Management",
    "section": "",
    "text": "Buy on Amazon\n\n\nThis part defines the core disciplines needed to turn fragmented records into a reliable organisational asset.\nFirst, it distinguishes data governance from information governance. The latter ensures lawful, secure handling; the former creates the conditions for use: shared definitions, ownership and stewardship, quality standards, metadata management, and master data for patients, providers and places. When organisations conflate the two, they achieve compliance but leave value on the table.\nSecond, it reframes data quality as fitness for purpose across multiple dimensions—accuracy, completeness, consistency, timeliness, uniqueness and validity. Different uses demand different thresholds: bedside decision-making needs currency and completeness; research needs consistent definitions, provenance and cohort clarity. Quality is dynamic, evolving as data travels through systems; therefore monitoring must be continuous, not a one-off cleanse.\nThird, it elevates metadata from documentation to semantic infrastructure. Descriptive definitions, structural relationships, administrative ownership, contextual capture (method, setting, units), and quality annotations together preserve meaning as data moves. Process models show metadata being created and consumed at every stage—from planning and capture to processing, analysis and sharing. Mature organisations progress from ad-hoc spreadsheets to metadata registries that drive validation, mapping and lineage.\nAcross these chapters, the practical message is consistent: make governance explicit (with a CDO and domain stewards), define and publish enterprise glossaries, adopt standards deliberately (and profile them), and embed quality/metadata checks into workflows. Do this and the organisation builds a reusable substrate for analytics, population health, research and AI—without reinventing the wheel each time."
  },
  {
    "objectID": "book/part4.html",
    "href": "book/part4.html",
    "title": "Part IV — Building the Infrastructure",
    "section": "",
    "text": "Buy on Amazon\n\n\nPart IV turns principles into architecture. It reviews the standards ecosystem—terminologies, classifications, information models, and exchange specs—and explains why success depends less on picking a winner and more on profiling, governance and tooling that keep meaning intact across vendors and regions.\nAt the centre is the case for metadata registries. A registry is not a passive catalogue; it’s an operational service that holds canonical definitions, value sets, constraints, mappings, ownership, and version history—then drives validation, integration and analytics. With a registry, teams resolve meaning questions once and reuse everywhere; without it, every project re-discovers the same ambiguities.\nThe part also restores a neglected competency: process documentation. Data inherits meaning from workflow. If we can’t describe the process that created the data—actors, steps, controls, and intent—secondary use becomes hazardous. Healthcare-adapted BPM, clinical concept frameworks, and lightweight catalogs make processes inspectable by people and machines, enabling safer automation and AI.\nThe architectural posture that emerges is modular and federated: local autonomy with shared semantics; profiles over pure theory; registries and process catalogs as the connective tissue; and implementation guides that specify “just enough” to be achievable by real NHS teams. It’s infrastructure you can actually build."
  },
  {
    "objectID": "book/part6.html",
    "href": "book/part6.html",
    "title": "Part VI — Case Studies & Implementation",
    "section": "",
    "text": "Buy on Amazon\n\n\nThe final part is deliberately practical. It extracts lessons from sectors that already treat data as infrastructure—statistics, supply chain, finance, digital government—and shows how healthcare can adapt their playbooks: common process models, registries of definitions, quality services, and federated governance that balances autonomy with coherence.\nIt then offers a maturity-based implementation approach. Start where foundations unlock the most value: clarify stewardship; stand up an enterprise glossary; implement a lightweight metadata registry for priority domains (e.g., medications, labs); add automated quality checks to pipelines; publish provenance with every dataset. Run short, instructive projects that create visible wins—such as a safer shared medications list or a reliable bed-capacity dashboard—while building reusable components.\nA shared-care-record pattern illustrates the method: profile standards; map local codes to shared value sets; document care processes; instrument quality monitors; and agree governance that outlives the project. The case studies emphasise change management: align incentives, bring clinicians into design, and measure outcomes that matter (errors avoided, time saved, decisions improved), not just interfaces built.\nThe part closes with future horizons: ecosystem-level governance, patient participation as data governors, and algorithmic assurance integrated into everyday operations. None of this requires waiting for perfect standards. It requires ownership of meaning, investment in metadata and quality, and disciplined, iterative delivery. That’s how health systems turn data into a strategic, trustworthy asset—one implementation at a time."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data as Foundation",
    "section": "",
    "text": "Buy on Amazon"
  },
  {
    "objectID": "index.html#vision",
    "href": "index.html#vision",
    "title": "Data as Foundation",
    "section": "Vision",
    "text": "Vision\nTo create healthcare information systems that are transparent, interoperable, and continuously learning. Data as Foundation envisions a future where every clinical, operational, and research decision is grounded in trusted data — seamlessly connected through shared meaning, robust governance, and intelligent architecture that evolves with healthcare itself."
  },
  {
    "objectID": "index.html#the-data-as-foundation-philosophy",
    "href": "index.html#the-data-as-foundation-philosophy",
    "title": "Data as Foundation",
    "section": "The Data as Foundation Philosophy",
    "text": "The Data as Foundation Philosophy\nData as Foundation is built on the belief that data is not a by-product of healthcare but its essential infrastructure. It proposes five interlocking principles.\n\nMeaning before technology: Shared semantics and metadata allow systems to change without losing understanding.\nGovernance as capability: Stewardship of data quality, provenance, and context ensures that information can be safely reused and repurposed.\nArchitecture as alignment: Enterprise architecture connects people, processes, and platforms around a coherent model of meaning rather than a single technology stack.\nMetadata as infrastructure: Registries and definitions provide the connective tissue linking data to its clinical and operational context.\nIntelligence through integrity: Trustworthy AI depends on trustworthy data — explainable, traceable, and grounded in evidence.\n\nTogether, these principles create an ecosystem where data flows confidently, decisions are transparent, and improvement becomes a property of the system itself."
  },
  {
    "objectID": "posts/2025-05-29-looking-back.html",
    "href": "posts/2025-05-29-looking-back.html",
    "title": "Looking Back: From Dyeline to Digital",
    "section": "",
    "text": "A story about the first seeds of information architecture on a construction site—and how those lessons map to health IT today."
  }
]