[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data as Foundation",
    "section": "",
    "text": "Buy on Amazon\nData As Foundation is both a philosophy and a call to action. It invites us to reimagine data as the shared foundation on which knowledge, trust, and progress are built. Across the public sector, the goal to create systems that are transparent, interoperable, and continuously learning These are systems that improve because they understand themselves."
  },
  {
    "objectID": "index.html#vision",
    "href": "index.html#vision",
    "title": "Data as Foundation",
    "section": "Vision",
    "text": "Vision\nWe envision a future where every clinical, operational, and civic decision is grounded in trusted data — connected by shared meaning, governed with integrity, and designed for reuse. Technology matters, but meaning comes first. When we understand what our data represents and how it should be governed, systems can evolve without losing coherence."
  },
  {
    "objectID": "index.html#the-philosophy",
    "href": "index.html#the-philosophy",
    "title": "Data as Foundation",
    "section": "The Philosophy",
    "text": "The Philosophy\nTechnology alone cannot create coherence. What connects organisations, professions, and decisions is shared meaning — the ability to describe, govern, and reuse data in context. By treating data as the foundation, we can build digital and analytical systems that are resilient, ethical, and adaptive.\n\n\n\n\n\n\nNoteA living approach\n\n\n\nData As Foundation is deliberately adaptable — a language and structure for aligning policy, design, and delivery around shared principles of meaning and governance."
  },
  {
    "objectID": "index.html#five-foundational-principles",
    "href": "index.html#five-foundational-principles",
    "title": "Data as Foundation",
    "section": "Five Foundational Principles",
    "text": "Five Foundational Principles\n\nMeaning before technology\nShared semantics and metadata allow systems to evolve without losing understanding.\nGovernance as capability\nStewardship of quality, provenance, and context ensures information is trustworthy and fit for reuse.\nArchitecture as alignment\nEnterprise architecture connects people, processes, and platforms around coherent models of meaning, not just technology stacks.\nMetadata as infrastructure\nRegistries and definitions form the connective tissue linking data to its clinical, operational, and organisational context.\nIntelligence through integrity\nTrusted AI and analytics depend on transparency, traceability, and evidence-based reasoning."
  },
  {
    "objectID": "index.html#a-foundation-for-public-value",
    "href": "index.html#a-foundation-for-public-value",
    "title": "Data as Foundation",
    "section": "A Foundation for Public Value",
    "text": "A Foundation for Public Value\nThe Data As Foundation approach extends beyond any single sector. It offers a shared framework for aligning policy, architecture, and governance around the same principles of meaning and trust. It helps organisations move from isolated projects to coherent, learning systems — capable of adapting to new evidence, new challenges, and new expectations.\n\nAlign policy, design, and delivery around explicit meaning.\n\nBuild governance that travels with the data.\n\nMeasure improvement by whether decisions become more reliable, explainable, and safe."
  },
  {
    "objectID": "index.html#a-living-framework",
    "href": "index.html#a-living-framework",
    "title": "Data as Foundation",
    "section": "A Living Framework",
    "text": "A Living Framework\nData As Foundation continues to evolve through research, dialogue, and real-world application. It brings together communities of architects, data professionals, and policy leaders who share a belief that progress begins with understanding, and that lasting trust is built from the ground up.\n\n\n\n\n\n\nTipGet involved\n\n\n\nUse the principles, adapt the patterns, and contribute improvements. Treat this as infrastructure for thinking and doing."
  },
  {
    "objectID": "book/part5.html",
    "href": "book/part5.html",
    "title": "Part V — Enabling the Future of Healthcare",
    "section": "",
    "text": "Buy on Amazon\n\nThis part connects foundations to the frontier. It starts with Foundations for AI: trustworthy systems require governed data, explicit semantics, lineage, and human-in-the-loop oversight. Generative tools add capability but increase the need for documented context, prompt/response traceability, and model risk management aligned to emerging standards.\nNext is Data Management Maturity. Digital maturity (features, adoption) is not the same as capability maturity (definitions, stewardship, quality, metadata, lifecycle). A practical maturity model lets organisations baseline where they are, prioritise gaps, and invest sequentially—often starting with stewardship, registries, and quality services that unlock downstream value faster than another app rollout.\nFinally, The Cultural Shift. Durable change is behavioural: moving from ownership to stewardship, from gatekeeping to clarity, and from individual heroics to systematic improvement. Leaders signal priorities by funding foundations, publishing definitions, and celebrating quality wins. Literacy matters too: clinicians and managers don’t need to be ontologists, but they do need shared language about meaning, quality and risk.\nThe message is hopeful: with foundations in place, AI augments clinical judgment, analytics informs planning, and information flows safely through a governed ecosystem. Without them, every innovation amplifies inconsistency."
  },
  {
    "objectID": "book/part3.html",
    "href": "book/part3.html",
    "title": "Part III — Rethinking Healthcare Data",
    "section": "",
    "text": "Buy on Amazon\n\nHere the vocabulary changes. Most so-called “reuse” of clinical data is actually repurposing—an active transformation from one context to another. That shift matters. Repurposing requires explicit assessment of original context, designed transformations, validation against the new purpose, and end-to-end provenance.\nThe part maps common scenarios: clinical-to-research (narrative to variables, cohort definition, de-identification); admin-to-quality (reinterpreting billing codes, aligning timestamps to clinical workflow); individual-to-population (aggregation, geocoding, enrichment with social data); and clinical-to-AI (labeling, bias checks, representativeness). Each demands different quality thresholds and exposes hidden biases if context is missing.\nIt then argues that data is a dynamic asset. Its value appreciates through use: relationships are formed, interpretations layered, quality refined, and metadata enriched. Treating data as “at rest” underestimates both risk and opportunity. Governance must therefore be evolutionary—tracking lineage, propagating corrections upstream, and reassessing value and quality as purposes change.\nThe takeaway is pragmatic: replace “copy once, use everywhere” with “transform deliberately, prove fitness.” Build shared transformation components, document them like code, measure semantic completeness (not just format compliance), and publish provenance with outputs. That’s how organisations move safely from siloed records to a learning system."
  },
  {
    "objectID": "book/part1.html",
    "href": "book/part1.html",
    "title": "Part I — The Healthcare Data Paradox",
    "section": "",
    "text": "Buy on Amazon\n\nPart I investigates why health systems repeatedly invest in digital programmes that promise transformation yet deliver only partial gains. The central claim is stark: we optimised visible technology while neglecting the invisible foundation—data governance, metadata, and shared semantics. The paradox is that we have standards, frameworks and budgets, yet meaning still fractures as information crosses organisational and system boundaries.\nThe narrative starts with NHS history: departmental systems in the 1990s, the ambition of NPfIT, and later waves favouring local innovation. Each era improved tooling, but too often assumed that consistent, high-quality data would naturally follow. In practice, variation in coding, local extensions, and undocumented context created a brittleness that no amount of messaging middleware could fix. Technical interoperability advanced faster than semantic interoperability.\nThe part names a persistent confusion: information governance (lawful, secure handling) is not data governance (stewardship of meaning, quality and lifecycle). Organisations passed audits yet struggled to use shared data safely because definitions, provenance and quality were unclear. COVID-era acceleration amplified this: where foundations existed, analytics and coordination flourished; where they didn’t, dashboards and decision support were hampered by inconsistency and missing context.\nA second thread is the metadata gap. Clinical terminologies (e.g., SNOMED CT), classifications and modern exchange standards are necessary but insufficient without operational metadata that records how data was captured, by whom, using what method, with which units and reference ranges. Without it, repurposing for research, quality, or AI becomes risky guesswork.\nPart I concludes with a call to action: treat data capability as distinct from digitisation. Establish senior data leadership; fund metadata infrastructure (registries, lineage, quality services); and measure data management maturity alongside system adoption. The lesson from decades of effort is not to spend less on digital—but to sequence and govern differently so technology sits on bedrock, not sand."
  },
  {
    "objectID": "book/overview.html",
    "href": "book/overview.html",
    "title": "About the Book",
    "section": "",
    "text": "Buy on Amazon\n\nData as Foundation: Building Healthcare’s Invisible Infrastructure argues that the real engine of digital health isn’t shiny apps or platforms—it’s the meaning baked into data and the governance that preserves it as information moves between contexts. Drawing on NHS history and international practice, the book shows why decades of digitisation have under-delivered: we automated processes but neglected the substrate that makes information trustworthy, reusable and safe.\nThe book is organised into six parts. Part I (The Healthcare Data Paradox) explains why large programmes faltered: they treated systems as ends, not means, and assumed good data would “emerge” from implementation. It introduces the blind spots around semantic consistency, metadata and true data governance. Part II (Fundamentals of Data Management) lays the core disciplines—data vs information governance, multidimensional data quality, and the pivotal role of metadata. It reframes quality as “fitness for purpose,” and metadata as operational infrastructure, not afterthought.\nPart III (Rethinking Healthcare Data) changes the vocabulary of improvement: most “data reuse” in health is actually repurposing that requires explicit transformation, provenance, and purpose-specific quality checks. It also treats data as a dynamic asset whose value and meaning evolve through use, relationships and enrichment. Part IV (Building the Infrastructure) moves from principles to architecture: standards ecosystems, metadata registries, and process documentation as the connective tissue for federated systems.\nLooking forward, Part V (Enabling the Future of Healthcare) shows that trustworthy AI depends on these same foundations—explainable models, auditable data, and an organisation that measures data management maturity alongside digital maturity. It also addresses the cultural work: leadership, stewardship and literacy. Finally, Part VI (Case Studies & Implementation) distils lessons from other sectors and offers practical, maturity-based steps for programmes such as Shared Care Records, while mapping future horizons like ecosystem governance and algorithmic assurance.\nAcross all six parts, the theme is constant: treat data as a managed, governed, semantic resource. Build registries and processes that preserve context. Separate compliance (information governance) from capability (data governance). Invest in metadata the way we invest in EPRs. Do this and digital health becomes steadily safer, more intelligent, and easier to change—because the foundation is sound."
  },
  {
    "objectID": "book/part2.html",
    "href": "book/part2.html",
    "title": "Part II — Fundamentals of Data Management",
    "section": "",
    "text": "Buy on Amazon\n\nThis part defines the core disciplines needed to turn fragmented records into a reliable organisational asset.\nFirst, it distinguishes data governance from information governance. The latter ensures lawful, secure handling; the former creates the conditions for use: shared definitions, ownership and stewardship, quality standards, metadata management, and master data for patients, providers and places. When organisations conflate the two, they achieve compliance but leave value on the table.\nSecond, it reframes data quality as fitness for purpose across multiple dimensions—accuracy, completeness, consistency, timeliness, uniqueness and validity. Different uses demand different thresholds: bedside decision-making needs currency and completeness; research needs consistent definitions, provenance and cohort clarity. Quality is dynamic, evolving as data travels through systems; therefore monitoring must be continuous, not a one-off cleanse.\nThird, it elevates metadata from documentation to semantic infrastructure. Descriptive definitions, structural relationships, administrative ownership, contextual capture (method, setting, units), and quality annotations together preserve meaning as data moves. Process models show metadata being created and consumed at every stage—from planning and capture to processing, analysis and sharing. Mature organisations progress from ad-hoc spreadsheets to metadata registries that drive validation, mapping and lineage.\nAcross these chapters, the practical message is consistent: make governance explicit (with a CDO and domain stewards), define and publish enterprise glossaries, adopt standards deliberately (and profile them), and embed quality/metadata checks into workflows. Do this and the organisation builds a reusable substrate for analytics, population health, research and AI—without reinventing the wheel each time."
  },
  {
    "objectID": "book/part4.html",
    "href": "book/part4.html",
    "title": "Part IV — Building the Infrastructure",
    "section": "",
    "text": "Buy on Amazon\n\nPart IV turns principles into architecture. It reviews the standards ecosystem—terminologies, classifications, information models, and exchange specs—and explains why success depends less on picking a winner and more on profiling, governance and tooling that keep meaning intact across vendors and regions.\nAt the centre is the case for metadata registries. A registry is not a passive catalogue; it’s an operational service that holds canonical definitions, value sets, constraints, mappings, ownership, and version history—then drives validation, integration and analytics. With a registry, teams resolve meaning questions once and reuse everywhere; without it, every project re-discovers the same ambiguities.\nThe part also restores a neglected competency: process documentation. Data inherits meaning from workflow. If we can’t describe the process that created the data—actors, steps, controls, and intent—secondary use becomes hazardous. Healthcare-adapted BPM, clinical concept frameworks, and lightweight catalogs make processes inspectable by people and machines, enabling safer automation and AI.\nThe architectural posture that emerges is modular and federated: local autonomy with shared semantics; profiles over pure theory; registries and process catalogs as the connective tissue; and implementation guides that specify “just enough” to be achievable by real NHS teams. It’s infrastructure you can actually build."
  },
  {
    "objectID": "book/part6.html",
    "href": "book/part6.html",
    "title": "Part VI — Case Studies & Implementation",
    "section": "",
    "text": "Buy on Amazon\n\nThe final part is deliberately practical. It extracts lessons from sectors that already treat data as infrastructure—statistics, supply chain, finance, digital government—and shows how healthcare can adapt their playbooks: common process models, registries of definitions, quality services, and federated governance that balances autonomy with coherence.\nIt then offers a maturity-based implementation approach. Start where foundations unlock the most value: clarify stewardship; stand up an enterprise glossary; implement a lightweight metadata registry for priority domains (e.g., medications, labs); add automated quality checks to pipelines; publish provenance with every dataset. Run short, instructive projects that create visible wins—such as a safer shared medications list or a reliable bed-capacity dashboard—while building reusable components.\nA shared-care-record pattern illustrates the method: profile standards; map local codes to shared value sets; document care processes; instrument quality monitors; and agree governance that outlives the project. The case studies emphasise change management: align incentives, bring clinicians into design, and measure outcomes that matter (errors avoided, time saved, decisions improved), not just interfaces built.\nThe part closes with future horizons: ecosystem-level governance, patient participation as data governors, and algorithmic assurance integrated into everyday operations. None of this requires waiting for perfect standards. It requires ownership of meaning, investment in metadata and quality, and disciplined, iterative delivery. That’s how health systems turn data into a strategic, trustworthy asset—one implementation at a time."
  }
]